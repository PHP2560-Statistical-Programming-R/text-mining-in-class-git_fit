---
title: "Text Analyis"
output: github_document
---

# Your mission

Perform text analysis.

## Okay, I need more information

Perform sentiment analysis or topic modeling using text analysis methods as demonstrated in the pre-class work and in the readings.

## Okay, I need even more information.

Do the above. Can't think of a data source?

- `gutenbergr`
- `AssociatedPress` from the `topicmodels` package
- `NYTimes` or `USCongress` from the `RTextTools` package
- Harry Potter Complete 7 Books text
    ```
    if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}

devtools::install_github("bradleyboehmke/harrypotter")
    ```
- [State of the Union speeches](https://pradeepadhokshaja.wordpress.com/2017/03/31/scraping-the-web-for-presdential-inaugural-addresses-using-rvest/)
- Scrape tweets using [`twitteR`](https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/)

Analyze the text for sentiment OR topic. **You do not need to do both**. The datacamp courses and [Tidy Text Mining with R](http://tidytextmining.com/) are good starting points for templates to perform this type of analysis, but feel free to *expand beyond these examples*.


# Timelines and Task

We will spend the next 2 weeks working on analyzing textual data in R. You will do the following:

- Start with some text based data.
- Clean data and prepare it for analysis
- Ask questions about the data
- Answer these questions with the data using tables and graphics
- Each group member must have their own unique question that they code the answer for. 


```{r, message=FALSE, warning=TRUE}
library(dplyr)
devtools::install_github("bradleyboehmke/harrypotter")
```



# Data cleaning:

```{r, message=FALSE, warning=TRUE}
library(tidytext)
library(dplyr)
library(stringr)
library(harrypotter)
library(ggplot2)
library(tidyr)
```


*Clean one book first and then generalize* 
```{r}
# Create tibble
data <- tibble(text = half_blood_prince) # column of text
data <- mutate(data, book = "Half Blood Prince") # define a new column for book name
data <- mutate(data, chapter = c(1:nrow(data))) # define a new column for chapter number

head(data)

data %>%
  unnest_tokens(word, text) # Transform the non-tidy text data to tidy text data
```


*Generalize this to all books to get dataset of the saga:*
```{r}
titles <- c("Philosopher's Stone", "Chamber of Secrets", "Prisoner of Azkaban",
            "Goblet of Fire", "Order of the Phoenix", "Half-Blood Prince",
            "Deathly Hallows")

books <- list(philosophers_stone, chamber_of_secrets, prisoner_of_azkaban,
           goblet_of_fire, order_of_the_phoenix, half_blood_prince,
           deathly_hallows)

saga <- tibble()

for (i in seq_along(titles)) { # create a tibble for each book
  data <- tibble(text = books[[i]]) %>% # add text
    mutate(book = titles[i]) %>% # define a new column for book name
    mutate(chapter = seq_along(books[[i]])) %>% # define a new column for chapter number
    group_by(book) %>% # group by book title
    unnest_tokens(word, text) %>%  # transform the non-tidy text data to tidy text data
    ungroup()
  
  saga <- rbind(saga, data) # bind rows of tibble of each book
}

saga

```

# Book 2: Chamber of Secrets
I will focus my analysis on the second book of the saga called "Chamber of Secrets"

```{r}
book2 <- saga %>%
  filter(book=="Chamber of Secrets")
```

## Group questions: 
(1) What is the word count for each book?   
```{r}
# Count the number of words per book
saga %>%
  group_by(book) %>%
  summarize(total.words = n())

# Number of words in the book "Chamber of Secrets"
nrow(book2)

```

(2) What is the word count of each chapter of the book "Chamber of Secrets"?
```{r}
book2 %>%
  group_by(chapter) %>%
  count()
```

(2) What are the most common words by chapter in the book "Chamber of Secrets"?

Besides articles and pronouns, the most common words are Harry, Ron, Hermione, Malfoy, and Lockhart, all of which are characters of the book. 

```{r}
book2 %>% 
  # Remove stop words like articles
  anti_join(stop_words) %>%
  group_by(chapter) %>%
  # Use count to find out the 10 words used the most in each chapter
  count(word, sort=TRUE) %>%
  top_n(10) %>%
  arrange(chapter)

# Plot 



```

(3) What are the most common words in the book "Chamber of Secrets"?
```{r}
common.words <- book2 %>% 
  # Remove stop words like articles
  anti_join(stop_words) %>%
  # Use count to find out the 10 words used the most in the book
  count(word, sort=TRUE) %>%
  top_n(10)

# Use aes() to put words on the x-axis and n on the y-axis
ggplot(common.words, aes(x=reorder(word, n), y=n)) +
  # Make a bar chart with geom_col()
  geom_col(fill="lightblue") +  
  coord_flip() + 
  ggtitle("Words Mentioned the Most in Chamber of Secrets") +
  theme(plot.title = element_text(hjust = 0.5)) 
```

(4) How does sentiment change across the entire book? 
```{r}
score.per.chapter <- book2 %>%
  # Count by chapter and word
  count(chapter, word, sort = TRUE) %>%
  # Implement sentiment analysis with the "afinn" lexicon
  inner_join(get_sentiments("afinn")) %>% 
  group_by(chapter) %>%
  # Find the net score for each chapter 
  summarise(net.score = sum(score))

score.per.chapter

# Plot 
ggplot(score.per.chapter, aes(x = chapter, y = net.score)) +
  geom_hline(aes(yintercept = 0, color="Reference line y=0"),size = 0.4) +
  geom_line(size=1, aes(color="Change of sentiment accross chapters")) +
  geom_smooth(method = "lm", se = FALSE, lty = 2, size = 0.7, aes(color="Lm smoothing")) +
  geom_smooth(method = "loess", se = FALSE, lty = 2, size = 0.7, aes(color="Loess smoothing")) +
  scale_x_continuous("Chapter", breaks=c(1:19)) +
  scale_y_continuous("Sentiment Score") +
  ggtitle("Sentiment change accross Chamber of Secrets") +
  theme(plot.title = element_text(hjust = 0.5)) 

```

```{r}
sentiment.count <- book2 %>%  
  # Implement sentiment analysis with the "bing" lexicon
  inner_join(get_sentiments("bing")) %>% 
  # Count the sentiment by chapter and negative
  group_by(chapter,sentiment) %>% 
  count(chapter,sentiment)  

sentiment.prop <-  sentiment.count %>%
  # Find the proportion of the total words that are positive and negative by chapter
  group_by(chapter) %>%
  mutate(total.words=sum(n), p = n/total.words)  

# Plot  
ggplot(sentiment.prop, aes(x = chapter, y = p, col = as.factor(sentiment))) +
  geom_line() 
```



(4) Which 10 characters are mentioned the most?
```{r}
# List of characters (not exhaustive)
characters <- tibble(word=c("harry", "ron", "hermione", "voldemort", "riddle", "hagrid", "dumbledore", "draco", "lucius", "ginny", "percy", "fred", "george", "molly", "arthur", "vernon", "petunia", "dudley", "minerva", "snape", "lockhart", "sprout", "filch", "norris", "cornelius", "oliver", "colin", "neville", "goyle", "crabbe", "millicent", "peeves", "hedwig", "aragog", "fang", "myrtle", "nick", "mason", "dobby", "mafalda", "errol", "perkins", "celestina", "christmas", "mundungus", "mortlake", "cannons", "martin", "borgin", "witch", "granger", "filibuster", "bozo", "hetty", "angus", "colin", "creevey", "banshee", "ogden", "gudgeon", "veronica", "patrick", "ghost", "widow", "ghost", "skower", "gryffindor", "hufflepuff", "ravenclaw", "slytherin", "yeti", "vampire", "werewolf", "ernie", "fawcett", "sinistra", "fawkes", "penelope", "mabel", "dwarf", "dippet", "aragog", "mosag", "hornby"))


no.stop.words <- book2 %>% 
  # Remove stop words like articles
  anti_join(stop_words)
  

top.characters <- no.stop.words %>%
  # Semi-join no.stop.words and characters to sort only words that match with character names
  semi_join(characters) %>%
  # Use count and top_n to find out the 10 characters that are mentioned the most
  count(word, sort=TRUE) %>%
  top_n(10) 

# Use aes() to put words on the x-axis and n on the y-axis
ggplot(top.characters, aes(x=reorder(word, n), y=n)) +
  # Make a bar chart with geom_col()
  geom_col(fill="salmon") +  
  coord_flip()
```



(5) How does the emotion "anticipation" change throughtout the book?
```{r}
anticipation1 <- book2 %>%
  # Implement sentiment analysis with the "nrc" lexicon
  inner_join(get_sentiments("nrc"))  %>%
  # Count the sentiment by chapter
  group_by(chapter) %>%
  count(sentiment) %>%
  # Filter sentiment anticipation
  filter(sentiment == "anticipation")

# Plot the change of the sentiment anticipation accross chapters
ggplot(anticipation1, aes(x=chapter, y=n)) +
geom_col(fill = "orange", width  = 0.7)

anticipation2 <- book2 %>%
  # Implement sentiment analysis with the "ncr" lexicon
  inner_join(get_sentiments("nrc"))  %>%
  # Count the words by chapter
  group_by(chapter, sentiment) %>% 
  count(word) %>%
  # Get the total occurrences per sentiment
  summarise(word.occurence = sum(n)) %>%
  # Get the total occurrences of all sentiments by chapter 
  mutate(total.words = sum(word.occurence)) %>%
  # Filter the sentiment anticipation
  filter(sentiment == "anticipation") %>%
  # Compute the proportion of occurrence by chapter
  mutate(proportion = word.occurence/total.words)
  
# Plot
ggplot(anticipation2, aes(x=chapter,y=proportion)) + 
  geom_line(color="orange")

```


## Individual questions: 

1. Which are the most common positive and negative words in the book "Chamber of Secrets"?
```{r}
word_counts <- book2 %>%
  # Implement sentiment analysis using the "bing" lexicon
  inner_join(get_sentiments("bing")) %>%
  # Count by word and sentiment
  count(word, sentiment)

top_words <- word_counts %>%
  # Group by sentiment
  group_by(sentiment) %>%
  # Take the top 10 for each sentiment
  top_n(10) %>%
  ungroup() %>%
  # Make word a factor in order of n
  mutate(word = reorder(word, n))

# Use aes() to put words on the x-axis and n on the y-axis
ggplot(top_words, aes(x=word, y=n, fill = sentiment)) +
  # Make a bar chart with geom_col()
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free") +  
  coord_flip()
```


2. Which words contribute the most to the negative sentiment and positive sentiment of the book "Chamber of Secrets"?
```{r}
book2 %>%
  # Implement sentiment analysis using the "afinn" lexicon
  inner_join(get_sentiments("afinn")) %>%
  # Select unique word and score columns
  select(word, score)%>%
  unique() %>%
  # Filter to only examine the scores that are negative
  filter(score<0) %>%
  arrange(score)

book2 %>%
  # Implement sentiment analysis using the "afinn" lexicon
  inner_join(get_sentiments("afinn")) %>%
  # Select unique word and score columns
  select(word, score)%>%
  unique() %>%
  # Filter to only examine the scores that are positive
  filter(score>0) %>%
  arrange(desc(score))

```

3. What is the relative contribution of each word to the sentiment of the book "Chamber of Secrets"? 
```{r}
book2 %>%
  # Count by word
  count(word, sort = TRUE) %>%
  # Implement sentiment analysis using the "afinn" lexicon
  inner_join(get_sentiments("afinn")) %>%
  # Filter to only examine the scores that are negative
  filter(score<0) %>%
  # Calculate a contribution for each word 
  mutate(contribution = (score*n)/sum(n)) %>%
  arrange(contribution) %>%
  ungroup()
    
book2 %>%
  # Count by word
  count(word, sort = TRUE) %>%
  # Implement sentiment analysis using the "afinn" lexicon
  inner_join(get_sentiments("afinn")) %>%
  # Filter to only examine the scores that are negative
  filter(score>0) %>%
  # Calculate a contribution for each word 
  mutate(contribution = (score*n)/sum(n)) %>%
  arrange(desc(contribution)) %>%
  ungroup()

```


If I were to start a book club with 10 of my friends who have not read or watch the movies of the Harry Potter Saga, what book should we begin reading? 


Find the percentage of negative words for each chapter.
```{r}
sentiment_counts <- book2 %>%
    # Implement sentiment analysis using the "bing" lexicon
    inner_join(get_sentiments("bing")) %>%
    # Count the number of words by title, type, and sentiment
    count(chapter, sentiment)

sentiment_counts %>%
    # Group by the chapter number
    group_by(chapter) %>%
    # Find the total number of words in each chapter
    mutate(total = sum(n),
    # Calculate the number of words divided by the total
           percent = n/total) %>%
    # Filter the results for only negative sentiment
    filter(sentiment=="negative") %>%
    arrange(desc(percent))
```


Find the percentage of positive words for each chapter.
```{r}
sentiment_counts <- book2 %>%
    # Implement sentiment analysis using the "bing" lexicon
    inner_join(get_sentiments("bing")) %>%
    # Count the number of words by title, type, and sentiment
    count(chapter, sentiment)

sentiment_counts %>%
    # Group by the chapter number
    group_by(chapter) %>%
    # Find the total number of words in each chapter
    mutate(total = sum(n),
    # Calculate the number of words divided by the total
           percent = n/total) %>%
    # Filter the results for only negative sentiment
    filter(sentiment=="positive") %>%
    arrange(desc(percent))
```


Calculate net sentiment: 
```{r}
library(tidyr)

book2 %>%
  inner_join(get_sentiments("bing")) %>%
  count(chapter, sentiment) %>%
  # Spread sentiment and n across multiple columns
  spread(sentiment, n, fill = 0) %>%
  # Use mutate to find net sentiment
  mutate(sentiment = positive-negative)

```


(3) How does sentiment change across the entire saga? 
```{r}
score.per.book <- saga %>%
  # Count by chapter and word
  count(book, word, sort = TRUE) %>%
  # Implement sentiment analysis with the "afinn" lexicon
  inner_join(get_sentiments("afinn")) %>% 
  group_by(book) %>%
  # Find the net score for each chapter 
  summarise(net.score = sum(score))

score.per.book

# Plot 
ggplot(score.per.book, aes(x = book, y = net.score)) +
  geom_hline(yintercept = 0, color="red", size=0.15, aes(color="Reference line y=0")) +
  geom_line(color="steelblue", size = 1, aes(color="Change of sentiment accross chapters")) +
  geom_smooth(method = "lm", se = FALSE, lty = 2, size = 0.5, color="purple", aes(color="Lm smoothing")) +
  geom_smooth(method = "loess", se = FALSE, lty = 2, size = 0.5, color="purple", aes(color="Lm smoothing")) +
  scale_x_continuous("Chapter", breaks=c(1:19)) +
  scale_y_continuous("Sentiment Score") +
  ggtitle("Sentiment change accross Chamber of Secrets") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  scale_color_manual(name='', values=c('Reference line y=0' = 'red', 'Change of sentiment accross chapters' = 'steelblue', 'Lm smoothing' = 'purple'))



```




















